{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ocg1SfAA-1AL"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip install transformers ray\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from ray import tune, put, get\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "Tfd5qgxuAVf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "NYeFLYhFwR-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "UPYuPQ-4tqYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_train_df = '/path/to/preprocessed/train_df.json'\n",
        "path_to_validation_df = '/path/to/preprocessed/val_df.json'\n",
        "\n",
        "train_df = pd.read_json(path_to_train_df)\n",
        "val_df = pd.read_json('/content/drive/My Drive/AI/SemEval2023/normalized/val_focused_no_augmentation_df.json')\n",
        "\n",
        "print(len(train_df))\n",
        "print(len(val_df))"
      ],
      "metadata": {
        "id": "VQxgeTJY-d_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LongformerTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
        "model = LongformerForSequenceClassification.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "OEBfAuYvAjp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(df):\n",
        "  MAX_LEN = 4096\n",
        "  input_token_ids = []\n",
        "  attention_mask_ids = []\n",
        "  label_ids = []\n",
        "\n",
        "  premise_list = df['Focused_premise'].to_list()\n",
        "  hypothesis_list = df['Statement'].to_list()\n",
        "  label_list = df['Label'].to_list()\n",
        "  label_dict = {'Entailment': 0, 'Contradiction': 1}\n",
        "\n",
        "  for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
        "    tokenization_output = tokenizer.encode_plus(text=hypothesis,\n",
        "                                                   text_pair=' '.join(premise),\n",
        "                                                   add_special_tokens=True,\n",
        "                                                   # padding=\"max_length\", # Somehow breaks tokenization. TODO: look into this\n",
        "                                                   truncation=True,\n",
        "                                                   max_length=MAX_LEN, \n",
        "                                                   return_tensors=\"pt\",\n",
        "                                                  #  return_token_type_ids=True, # no segment ids for this model \n",
        "                                                   return_attention_mask=True)\n",
        "      \n",
        "    out_input_ids = tokenization_output['input_ids'][0]\n",
        "    out_mask_ids = tokenization_output['attention_mask'][0]\n",
        "\n",
        "    input_token_ids.append(out_input_ids)\n",
        "    attention_mask_ids.append(out_mask_ids)\n",
        "    label_ids.append(label_dict[label])\n",
        "\n",
        "  input_token_ids = pad_sequence(input_token_ids, batch_first=True)\n",
        "  attention_mask_ids = pad_sequence(attention_mask_ids, batch_first=True)\n",
        "  label_ids = torch.tensor(label_ids)\n",
        "  \n",
        "  dataset = TensorDataset(input_token_ids, attention_mask_ids, label_ids)\n",
        "  return dataset\n",
        "\n",
        "# Get the train, validation dataframes. For now, just return em' \n",
        "def get_train_val_dfs():\n",
        "  return train_df, val_df\n",
        "\n",
        "def get_dataloaders(batchSize):\n",
        "  train_df, validation_df = get_train_val_dfs()\n",
        "  \n",
        "  train_data = get_dataset(train_df)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batchSize)\n",
        "\n",
        "  validation_data = get_dataset(validation_df)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batchSize)\n",
        "\n",
        "  return train_dataloader, validation_dataloader"
      ],
      "metadata": {
        "id": "n2NhwpNaAi3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(train_df[\"Premise\"])"
      ],
      "metadata": {
        "id": "ktZtB3xcQ5gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_dataset(val_df)"
      ],
      "metadata": {
        "id": "ZVDp91P5QFZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t = tokenizer.encode(\"Inclusion Criteria:   Pathologically confirmed breast cancer, determined to be a candidate for primary systemic (neoadjuvant) therapy and for surgical resection of residual primary tumor following completion of neoadjuvant therapy   Locally advanced breast cancer, not stage IV, and with a tumor size >= 2 cm (as measured on imaging or estimated by physical exam)   No obvious contraindications for primary chemotherapy   Residual tumor planned to be removed surgically following completion of neoadjuvant therapy   Able to lie still for 1.5 hours for PET scanning   Eastern Cooperative Oncology Group (ECOG) performance status =< 2 (Karnofsky >= 60%)   Leukocytes >= 3,000/ul   Absolute neutrophil count >= 1,500/ul   Platelets >= 100,000/ul   Total bilirubin within normal institutional limits   Aspartate aminotransferase (AST) (serum glutamic oxaloacetic transaminase [SGOT])/alanine aminotransferase (ALT) (serum glutamate pyruvate transaminase [SGPT]) =< 2.5 times the institutional upper limit of normal   Creatinine within normal institutional limits OR creatinine clearance >= 30 mL/min/1.73 m^2 for patients with creatinine levels above institutional normal   If female, postmenopausal for a minimum of one year, OR surgically sterile, OR not pregnant, confirmed by institutional standard of care (SOC) pregnancy test, and willing to use adequate contraception (hormonal or barrier method of birth control; abstinence) for the duration of study participation   Able to understand and willing to sign a written informed consent document and a Health Insurance Portability and Accountability Act (HIPAA) authorization in accordance with institutional guidelines Exclusion Criteria:   Previous treatment (chemotherapy, radiation, or surgery) to involved breast; including hormone therapy   Uncontrolled intercurrent illness including, but not limited to, ongoing or active infection, symptomatic congestive heart failure, unstable angina pectoris, cardiac arrhythmia, or psychiatric illness/social situations that would limit compliance with study requirements   Medically unstable   Condition requiring anesthesia for PET scanning and/or unable to lie still for 1.5 hours   History of allergic reactions attributed to compounds of similar chemical or biologic composition to F-18 fluorothymidine   Pregnant or nursing   Previous malignancy, other than basal cell or squamous cell carcinoma of the skin or in situ carcinoma of the cervix, from which the patient has been disease free for less than 5 years   Currently on hormone therapy as the primary systemic neoadjuvant therapy\")\n",
        "# len(t)"
      ],
      "metadata": {
        "id": "65hug9kWSI-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training epochs\n",
        "epochs = 5\n",
        "\n",
        "def get_scheduler_and_optimizer(model, learningRate, batches, adamEpsilon = 1e-8, warmupStepsPercentage=0):\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                  lr = learningRate, # args.learning_rate - default is 5e-5\n",
        "                  eps = adamEpsilon # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "  \n",
        "  total_steps = epochs * batches\n",
        "\n",
        "  return get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = math.floor(total_steps * warmupStepsPercentage),\n",
        "                                            num_training_steps = total_steps), optimizer"
      ],
      "metadata": {
        "id": "rpTOrqFoGkgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "n8tOnCU2Gw5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(config, model, checkpoint_dir=None):\n",
        "  # Store the average loss after each epoch so we can plot them.\n",
        "  loss_values = []\n",
        "\n",
        "  evaluated_model_ref = get(model)\n",
        "\n",
        "  train_dataloader, validation_dataloader = get_dataloaders(config[\"batchSize\"]) # 300 and 2 are just placeholder values\n",
        "  scheduler, optimizer = get_scheduler_and_optimizer(evaluated_model_ref, config[\"learningRate\"], len(train_dataloader) * epochs, config[\"adamEpsilon\"], config[\"warmupStepsPercentage\"])\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  if checkpoint_dir:\n",
        "    model_state, optimizer_state = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "    evaluated_model_ref.load_state_dict(model_state)\n",
        "    optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      evaluated_model_ref.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, (pair_token_ids, mask_ids, labels) in enumerate(train_dataloader):\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          evaluated_model_ref.zero_grad()   \n",
        "\n",
        "          pair_token_ids = pair_token_ids.to(device)\n",
        "          mask_ids = mask_ids.to(device)\n",
        "          labels = labels.to(device)     \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # This will return the loss (rather than the model output) because we\n",
        "          # have provided the `labels`.\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          loss, prediction = evaluated_model_ref(input_ids=pair_token_ids, \n",
        "                                  attention_mask=mask_ids, \n",
        "                                  labels=labels).values()\n",
        "          \n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(evaluated_model_ref.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      evaluated_model_ref.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for (pair_token_ids, mask_ids, labels) in validation_dataloader:\n",
        "          \n",
        "          # Add batch to GPU\n",
        "          pair_token_ids = pair_token_ids.to(device)\n",
        "          mask_ids = mask_ids.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Telling the model not to compute or store gradients, saving memory and\n",
        "          # speeding up validation\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have\n",
        "              # not provided labels.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              outputs = evaluated_model_ref(input_ids=pair_token_ids, \n",
        "                              attention_mask=mask_ids)\n",
        "          \n",
        "          # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "          # values prior to applying an activation function like the softmax.\n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "          # # dubug only - get the incorrectly classified token sequences.\n",
        "          # pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "          # labels_flat = label_ids.flatten()\n",
        "          # token_ids_list = pair_token_ids.cpu().numpy()\n",
        "          # incorrectly_classified.append(token_ids_list[pred_flat != labels_flat])\n",
        "          \n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "          tmp_eval_loss = criterion(torch.tensor(logits), torch.tensor(label_ids))\n",
        "          \n",
        "          # Accumulate the total accuracy.\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "          eval_loss += tmp_eval_loss\n",
        "\n",
        "          # Track the number of batches\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      with tune.checkpoint_dir(epoch_i) as checkpoint_dir:\n",
        "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "        torch.save((evaluated_model_ref.state_dict(), optimizer.state_dict()), path)\n",
        "            \n",
        "      tune.report(loss=(eval_loss/nb_eval_steps), accuracy=eval_accuracy/nb_eval_steps)"
      ],
      "metadata": {
        "id": "648XWXdRGx3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# driver\n",
        "\n",
        "config = {\n",
        "    \"learningRate\": tune.choice([2e-5]),\n",
        "    \"batchSize\": tune.choice([8]),\n",
        "    \"adamEpsilon\": tune.choice([1e-8]),\n",
        "    \"warmupStepsPercentage\": tune.choice([0])\n",
        "    }\n",
        "    #   # \"learningRate\": tune.loguniform(1e-5, 5e-5),\n",
        "    #   # \"learningRate\": tune.choice([2e-5, 3e-4, 1e-4, 5e-5, 3e-5, 2e-3, 2e-2, 2e-1]),\n",
        "\n",
        "# config = {\n",
        "#       \"windowSize\": tune.choice([100, 150]),\n",
        "#       \"windowOverlapRate\": tune.choice([1.25, 2, 3]),\n",
        "#       \"learningRate\": tune.loguniform(2e-5, 3e-5),\n",
        "#       # \"learningRate\": tune.choice([2e-5, 3e-4, 1e-4, 5e-5, 3e-5, 2e-3, 2e-2, 2e-1]),\n",
        "#       \"batchSize\": tune.choice([64]),\n",
        "#       \"adamEpsilon\": tune.choice([1e-8, 1e-6]),\n",
        "#       \"warmupStepsPercentage\": tune.choice([0, 0.02, 0.05, 0.1, 0.2, 0.35])\n",
        "#     }\n",
        "\n",
        "scheduler = ASHAScheduler(\n",
        "    metric=\"accuracy\",\n",
        "    mode=\"max\",\n",
        "    max_t=epochs,\n",
        "    grace_period=2,\n",
        "    reduction_factor=2)\n",
        "\n",
        "reporter = CLIReporter(\n",
        "    metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "\n",
        "# Create a ref to the model to pass to the fine_tune function.\n",
        "# The model is ~600 MiB. If it is directly referenced in our fine_tune method, Ray will throw a 'worker function too big > 95MiB' error.\n",
        "modelRef = put(model)\n",
        "\n",
        "result = tune.run(\n",
        "    partial(fine_tune, model=modelRef),\n",
        "    resources_per_trial={\"gpu\": 1},\n",
        "    config=config,\n",
        "    num_samples=1,\n",
        "    scheduler=scheduler,\n",
        "    progress_reporter=reporter)\n",
        "\n",
        "best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
        "print(\"Best trial config: {}\".format(best_trial.config))\n",
        "print(\"Best trial final validation loss: {}\".format(\n",
        "    best_trial.last_result[\"loss\"]))\n",
        "print(\"Best trial final validation accuracy: {}\".format(\n",
        "    best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "# best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "# device = \"cpu\"\n",
        "# if torch.cuda.is_available():\n",
        "#     device = \"cuda:0\"\n",
        "#     if gpus_per_trial > 1:\n",
        "#         best_trained_model = nn.DataParallel(best_trained_model)\n",
        "# best_trained_model.to(device)\n",
        "\n",
        "# best_checkpoint_dir = best_trial.checkpoint.value\n",
        "# model_state, optimizer_state = torch.load(os.path.join(\n",
        "#     best_checkpoint_dir, \"checkpoint\"))\n",
        "# best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "# test_acc = test_accuracy(best_trained_model, device)\n",
        "# print(\"Best trial test set accuracy: {}\".format(test_acc))"
      ],
      "metadata": {
        "id": "UkVwlMHcIFrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
        "result.get_best_checkpoint(trial, \"accuracy\", \"max\", \"last\")"
      ],
      "metadata": {
        "id": "rY7PNqZ5JBhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}