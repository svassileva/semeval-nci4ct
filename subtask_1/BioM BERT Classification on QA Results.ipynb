{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification"
      ],
      "metadata": {
        "id": "_gpgWjGUXqb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "USoTGuKHrDuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'sultan/BioM-BERT-PubMed-PMC-Large'\n",
        "task_2_results_df_path = '/path/to/task2/results/df.json'\n",
        "output_path = '/output/results/path/output.json'\n",
        "checkpoint_path = '/path/to/finetuned/model/checkpoint'"
      ],
      "metadata": {
        "id": "F0K168cD4Esv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_2_results_df = pd.read_json(task_2_results_df_path) # load the results from task 2 (evidence sentence indexes for each input example)"
      ],
      "metadata": {
        "id": "reaCaVLr67nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = ElectraForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
        "tokenizer = ElectraTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model_state, optimizer_state = torch.load(checkpoint_path)\n",
        "bert_model.load_state_dict(model_state)\n",
        "\n",
        "bert_model.to(device)"
      ],
      "metadata": {
        "id": "0a85oAOhrM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "label_dict = {0 : 'Entailment', 1 : 'Contradiction'}\n",
        "\n",
        "def get_prediction(row):\n",
        "  hypothesis = row['Statement']\n",
        "  section_id = row['Section_id']\n",
        "\n",
        "  model = bert_model \n",
        " \n",
        "  tokenization_output = tokenizer.encode_plus(text=hypothesis,\n",
        "                                                   text_pair=' '.join(row['Premise']), # premise is a list of evidence sentences\n",
        "                                                   add_special_tokens=True,\n",
        "                                                   truncation=True,\n",
        "                                                   max_length=MAX_LEN, \n",
        "                                                   return_tensors='pt',\n",
        "                                                   return_token_type_ids=True,\n",
        "                                                   return_attention_mask=True)\n",
        "    \n",
        "  out_input_ids = tokenization_output['input_ids'][0]\n",
        "  out_mask_ids = tokenization_output['attention_mask'][0]\n",
        "  out_segment_ids = tokenization_output['token_type_ids'][0]\n",
        "\n",
        "  # model.forward expects a tensor of shape (batch_size, sample_length).\n",
        "  # The batch size here is 1, but the shape is (sample_length), so we have to reshape\n",
        "  input_len = out_input_ids.size(0)\n",
        "  out_input_ids = torch.reshape(out_input_ids, (1, input_len)).cuda()\n",
        "  out_mask_ids = torch.reshape(out_mask_ids, (1, input_len)).cuda()\n",
        "  out_segment_ids = torch.reshape(out_segment_ids, (1, input_len)).cuda()\n",
        "\n",
        "  outputs = model(input_ids=out_input_ids, \n",
        "                  attention_mask=out_mask_ids,\n",
        "                  token_type_ids=out_segment_ids)\n",
        "  \n",
        "  logits = outputs[0].detach().cpu()\n",
        "\n",
        "  prediction_probabilities = torch.sigmoid(logits)\n",
        "  classification = torch.argmax(prediction_probabilities).item() \n",
        "  return label_dict[classification]"
      ],
      "metadata": {
        "id": "sl3r2TN51A0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_2_results_df['Prediction'] = task_2_results_df.apply(lambda row: get_prediction(row), axis=1)"
      ],
      "metadata": {
        "id": "VgSLSq4r8rTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = qa_dev_results_df[['Prediction']].copy() # the official format requires only example ID + predicted label\n",
        "result_df.head()"
      ],
      "metadata": {
        "id": "MSpQrMy2Hfcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.to_json(output_path, orient ='index')"
      ],
      "metadata": {
        "id": "gOJceg_z923R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}